{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk transformers torch scikit-learn --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2116016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43339021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load dataset \n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv(\"hf://datasets/stepp1/tweet_emotion_intensity/train.csv\")\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Import the `re` module for working with regular expressions\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert all text to lowercase for uniformity\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
    "    text = re.sub(r'<[^>]*>', '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
    "    return text # Return the cleaned text\n",
    "\n",
    "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
    "# Apply the cleaning function to each row of the 'text' column\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
    "print(data['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e932809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum()) # Print the count of missing values for each column\n",
    "\n",
    "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
    "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
    "\n",
    "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
    "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c46593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the dataset to identify the label column\n",
    "print(\"Dataset columns:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Extract input_ids and attention_masks from tokens\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# Create labels from the 'labels' column in the dataset\n",
    "labels = torch.tensor(data['labels'].values)\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, num_workers=0)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
