{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install loralib --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pre-trained BERT model for classification tasks\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Print model layers to identify attention layers where LoRA can be applied\n",
    "for name, module in model.named_modules():\n",
    "    print(name)  # This output helps locate attention layers\n",
    "\n",
    "# Replace linear layers in attention with LoRA layers\n",
    "for name, module in model.named_modules():\n",
    "    if 'attention' in name and isinstance(module, nn.Linear):\n",
    "        # Replace the linear layer with a LoRA linear layer\n",
    "        parent_name = '.'.join(name.split('.')[:-1])\n",
    "        child_name = name.split('.')[-1]\n",
    "        parent_module = model.get_submodule(parent_name) if parent_name else model\n",
    "        setattr(parent_module, child_name, lora.Linear(module.in_features, module.out_features, r=8))\n",
    "\n",
    "# Mark only LoRA parameters as trainable\n",
    "lora.mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset (example using a sample dataset)\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load a sample dataset (you can replace this with your own dataset)\n",
    "# For this example, we'll use a sentiment analysis dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))  # Using subset for faster training\n",
    "val_data = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "test_data = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(200, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109621e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Configure training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Set up the Trainer to handle fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "# Begin training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the LoRA fine-tuned model on the test set\n",
    "results = trainer.evaluate(eval_dataset=test_data)\n",
    "print(f\"Test results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: To adjust the rank in LoRA, you would modify the 'r' parameter when creating LoRA layers\n",
    "# The rank was set to 8 in cell 2. To use a different rank (e.g., 2, 4, 16), \n",
    "# you would need to recreate the model with the desired rank value in the lora.Linear() initialization\n",
    "\n",
    "# Display the number of trainable parameters in the LoRA-adapted model\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
