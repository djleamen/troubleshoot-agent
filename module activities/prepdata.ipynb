{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528c1295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/djleamen/Library/Python/3.12/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/djleamen/Library/Python/3.12/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.12/site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk transformers torch scikit-learn --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2116016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/djleamen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/djleamen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43339021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet    class  \\\n",
      "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
      "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
      "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
      "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
      "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
      "\n",
      "  sentiment_intensity class_intensity  labels  \n",
      "0                 low        fear_low       4  \n",
      "1                high    sadness_high       9  \n",
      "2              medium     fear_medium       5  \n",
      "3                high      anger_high       0  \n",
      "4                 low        fear_low       4  \n"
     ]
    }
   ],
   "source": [
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load dataset \n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv(\"hf://datasets/stepp1/tweet_emotion_intensity/train.csv\")\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaf5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    loved bethenny independence msg on wendywillia...\n",
      "1    mark_slifer actually maybe we were supposed to...\n",
      "2    i thought the nausea and headaches had passed ...\n",
      "3    anger resentment and hatred are the destroyer ...\n",
      "4      new tires amp an alarm system on my car fwm now\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re # Import the `re` module for working with regular expressions\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert all text to lowercase for uniformity\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
    "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
    "    return text # Return the cleaned text\n",
    "\n",
    "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
    "# Apply the cleaning function to each row of the 'text' column\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
    "print(data['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e932809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "tweet                  0\n",
      "class                  0\n",
      "sentiment_intensity    0\n",
      "class_intensity        0\n",
      "labels                 0\n",
      "cleaned_text           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/fnl461r570n3d_hfz1jxb1xh0000gn/T/ipykernel_35145/3284956397.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum()) # Print the count of missing values for each column\n",
    "\n",
    "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
    "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
    "\n",
    "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
    "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c46593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n",
      "         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n",
      "         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n",
      "          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n",
      "          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n",
      "          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n",
      "          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebe7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e455bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word \n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdaa0ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns:\n",
      "['id', 'tweet', 'class', 'sentiment_intensity', 'class_intensity', 'labels', 'cleaned_text', 'augmented_text']\n",
      "\n",
      "First few rows:\n",
      "      id                                              tweet    class  \\\n",
      "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
      "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
      "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
      "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
      "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
      "\n",
      "  sentiment_intensity class_intensity  labels  \\\n",
      "0                 low        fear_low       4   \n",
      "1                high    sadness_high       9   \n",
      "2              medium     fear_medium       5   \n",
      "3                high      anger_high       0   \n",
      "4                 low        fear_low       4   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  loved bethenny independence msg on wendywillia...   \n",
      "1  mark_slifer actually maybe we were supposed to...   \n",
      "2  i thought the nausea and headaches had passed ...   \n",
      "3  anger resentment and hatred are the destroyer ...   \n",
      "4    new tires amp an alarm system on my car fwm now   \n",
      "\n",
      "                                      augmented_text  \n",
      "0  loved bethenny independence msg on wendywillia...  \n",
      "1  mark_slifer actually possibly we were supposed...  \n",
      "2  i intend the nausea and headaches suffer passe...  \n",
      "3  anger resentment and hatred are the destroyer ...  \n",
      "4  new tires adenosine_monophosphate an alarm sys...  \n"
     ]
    }
   ],
   "source": [
    "# Check the columns in the dataset to identify the label column\n",
    "print(\"Dataset columns:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40f15dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and test sets are prepared with attention masks!\n",
      "Number of training samples: 2692\n",
      "Number of validation samples: 674\n",
      "Number of test samples: 594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Extract input_ids and attention_masks from tokens\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# Create labels from the 'labels' column in the dataset\n",
    "labels = torch.tensor(data['labels'].values)\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
